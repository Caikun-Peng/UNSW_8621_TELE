---
Owner: Peng, Caikun
Electives: Level 4
Last edited time: 2024-08-07
Course Progress: In Progress
Note Progress: In Progress
Note ID: TELE4642-Note-1
---

Content
- [Probability Theory](#probability-theory)
  - [Probability](#probability)
    - [Conditional Probability](#conditional-probability)
  - [Random Variables](#random-variables)
    - [Continuous random variables](#continuous-random-variables)
    - [Discrete random variables](#discrete-random-variables)
  - [Mean / Moments](#mean--moments)
- [Stochastic Process](#stochastic-process)
  - [Markov Process](#markov-process)
  - [Poisson Process](#poisson-process)
- [Queuing systems](#queuing-systems)

---

# Probability Theory
## Probability
- Terminology
  - Random experiment  
    An experiment for which the result is not known a priori
  - Sample point  
    A possible outcome
  - Sample space $S$  
    The set of all sample points
  - Event $A$  
    A subset of $S$  
- Definition  
  A probability measure defined on $S$ is a 
  function that associates to each possible 
  event $A$ a real number $P(A)$ such that:  
  1. $0 \le P(A) \le 1$  
  2. $P(S) = 1$  
  3. $P(\cup_{i=1}^{\infty} A_i) = \sum_{i=1}^{\infty} P(A_i)$ if $A_i$'s mutually exclusive
### Conditional Probability
- Definition  
  The conditional probability of event $A$ given that event $B$ has occurred, denoted by $P(A|B)$, is defined as  
  $P(A|B) = \dfrac{P(A\text{ and }B)}{P(B)}$
- Bayes theorem  
  Given a partition $\{G_1, G_2, ...\}$ and event $E$   
  $P(G_i|E) = \dfrac{P(E|G_i)P(G_i)}{\sum_{j}P(E|G_j)P(G_j)}$
- Independent Events  
  Two events are statistically independent if  
  $P(A|B) = P(A)$ and $P(B|A) = P(B)$
## Random Variables  
- Definition
  A random variable $X$ maps each outcome $s$ 
  in the sample space $S$ to a real number $X(s)$
### Continuous random variables
- Can take on an uncountably infinite 
  number of distinct values
- Cumulative distribution function (CDF):  
  $F(x) = P(X \le x)$
- Probability density function (PDF):  
  $f(x) = \dfrac{dF(x)}{dx}$ and 
  $F(X) = \int_{-\infty}^{x} f(y)dy$
- Exponential r.v.
  - PDF: $f(x) = \mu e^{-\mu x},x\ge0$
  - CDF: $F(x) = 1 - e^{-\mu x}$
### Discrete random variables
- Can take on a finite or countably infinite 
  number of values  
  $p(x) = P[X=x]$
- Cumulative distribution function (CDF):  
  $F(x) = P(X \le x) = \displaystyle\sum_{t=x_{min}}^{x} P(X=t)$
## Mean / Moments
- The mean (or expected value or expectation or first moment) 
  of a continuous r.v. is  
  $E[X] = \int_{-\infty}^{\infty}xf(x)dx$
- The mean of a discrete r.v. is  
  $E[X] = \sum_{\forall k}k P(X=k)$

# Stochastic Process
- Definition
  A stochastic process (or random process) is a family of random variables $\{X(t): t \in T\}$ indexed by parameter $t$ over index set $T$
## Markov Process
- Definition  
  A stochastic process having the Markov property  
  $P(X_{s+t} = j | X_u = x_u; u \le s) = P(X_{s+t} = j | X_s = x_s)$
  - Sample path followed by the process after any time $t$ depends only on the state $X_t$ existing at that time, and not on past history (memoryless)
  - We consider: continuous-time, discrete-value
- Important Markov process: **Poisson process**
  - Simplest, mathematically well-behaved example of a Markov process
  - It is a counting process for the number of randomly occurring point-events observed in a given interval of time
## Poisson Process
- Definition  
  - Let the r.v. $N(t,\tau)$ denote number of arrivals in $(t,\tau]$  
  - Let $o(h)$ denotes an expression $f(h)$ such that:  
  - A Possion process $\{N(0,t)\}, t \ge 0$, with rate $\lambda$ is such that: 
    1. $P[N(t,t+h)=0] = 1 - \lambda h + o(h)$
    2. $P[N(t,t+h)=1] = \lambda h + o(h)$
    3. $P[N(t,t+h)=2] = o(h)$
    4. $N(0,t)$ and $N(t,t+h)$ are independent for all $t,h>0$  
  - Every small interval has equal likelihood of arrival occurrence  
    - Arrivals are thus completely random in time
  - Time-homogeneity property:  
    for all $\tau$: $P[N(\tau, \tau+t)] = P[N(0,t)] = P[N(t)]$
- PMF
  - The number of arrivals $N(t)$ in any interval of length $t$ is a Poisson r.v. with parameter $\lambda t$:  
    $P[N(t)=k] = \dfrac{e^{-\lambda t}(\lambda t)^k}{k!}$
  - Mean number of arrivals in any interval of length $t$:  
    $E[N(t)] = \sum_{k=0}^{\infty} k P[N(t)=k] = \lambda t$  
  - Poisson is a limiting approximation of binomial ($\lambda t = np$): 
    $\lim_{n\to\infty}\left(\begin{matrix}n\\i\end{matrix}\right)p^{i} (1-p)^{n-i} = e^{-\lambda t} \dfrac{(\lambda t)^i}{i!}$
  - Inter-arrival time (i.e. time between successive arrivals) is an exponential r.v. with parameter $\lambda$
    - Time to first arrival is exponential 
    - Time to next arrival is exponential
- Superposition  
  $k$ independent Possion process $A_1,...,A_k$ with rate $\lambda_1,...,\lambda_k$ are merged inte a single process, $A=A_1+\cdots+A_k$, the combined process $A$ is Possion with rate $\lambda = \lambda_1+\cdots+\lambda_k$
- Decomposition  
  A Possion process with rate $\lambda$ is split into $k$ process independently with $i$-th process having probability $p_i$ where $p_1+\cdots+p_k = 1$.  
  The $k$ processes are each Possion with rate $p_i \lambda$ for the $i$-th process.

# Queuing systems

## Little's Result

The mean number of customers in the system equals the product of the arrival rate and the mean sojourn time in the system
$$
E[n]=\lambda E[\tau]\\
N=\lambda T
$$





---
[Back: Network Performance Overview](<0. TELE4642 Network Performance Overview.md>)

[Next: M/M/1 Queues](<2. TELE4642 Lecture 2 MM1 Queues.md>)